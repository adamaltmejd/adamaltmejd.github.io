- id: camerer2016
  type: Publications
  project: Reproducibility
  title: Evaluating Replicability of Laboratory Experiments in Economics
  coauthors:
    - name: Colin F. Camerer
      site: http://www.its.caltech.edu/~camerer/camerer.html
    - name: Anna Dreber
      site: https://sites.google.com/site/annadreber/
    - name: Eskil Forsell
    - name: Teck Ho
    - name: Juergen Huber
      site: https://www.uibk.ac.at/ibf/mitarbeiter/huberj.html
    - name: Magnus Johannesson
    - name: Michael Kirchler
      site: https://www.uibk.ac.at/ibf/team/kirchler.html.en
    - name: Johan Almenberg
    - name: Adam Altmejd
    - name: Taizan Chan
    - name: Emma Heikensten
    - name: Felix Holzmeister
    - name: Taisuke Imai
    - name: Siri Isaksson
    - name: Gideon Nave
      site: https://marketing.wharton.upenn.edu/profile/gnave/
    - name: Thomas Pfeiffer
    - name: Michael Razen
    - name: Hang Wu
  journal: Science
  year: 2016
  doi: 10.1126/science.aaf0918
  abstract: "The replicability of some scientific findings has recently been called into question. To contribute data about replicability in economics, we replicated 18 studies published in the <i>American Economic Review</i> and the <i>Quarterly Journal of Economics</i> between 2011 and 2014. All of these replications followed predefined analysis plans that were made publicly available beforehand, and they all have a statistical power of at least 90% to detect the original effect size at the 5% significance level. We found a significant effect in the same direction as in the original study for 11 replications (61%); on average, the replicated effect size is 66% of the original. The replicability rate varies between 67% and 78% for four additional replicability indicators, including a prediction market measure of peer beliefs."
  coverage:
    - outlet: Last Week Tonight with John Oliver
      url: https://www.youtube.com/watch?v=0Rnq1NpHdmw
    - outlet: The Economist
      url: https://www.economist.com/science-and-technology/2016/03/05/a-far-from-dismal-outcome
    - outlet: Science Mag
      url: https://www.sciencemag.org/news/2016/03/about-40-economics-experiments-fail-replication-survey
    - outlet: The Chronicle of Higher Education
      url: https://www.chronicle.com/article/Can-Science-s/235582

- id: camerer2018
  type: Publications
  project: Reproducibility
  title: Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015
  journal: Nature Human Behavior
  year: 2018
  doi: 10.1038/s41562-018-0399-z
  ungated: https://rdcu.be/6VWt
  coauthors:
    - name: Colin F. Camerer
      site: http://www.its.caltech.edu/~camerer/camerer.html
    - name: Anna Dreber
    - name: Felix Holzmeister
    - name: Teck Ho
    - name: Juergen Huber
    - name: Magnus Johannesson
    - name: Michael Kirchler
      site: https://www.uibk.ac.at/ibf/team/kirchler.html.en
    - name: Gideon Nave
      site: https://marketing.wharton.upenn.edu/profile/gnave/
    - name: Brian A. Nosek
    - name: Thomas Pfeiffer
    - name: Nick Buttrick
    - name: Taizan Chan
    - name: Yiling Chen
    - name: Eskil Forsell
    - name: Anup Gampa
    - name: Emma Heikensten
    - name: Lily Hummer
    - name: Taisuke Imai
    - name: Siri Isaksson
    - name: Dylan Manfredi
    - name: Julia Rose
    - name: Eric-Jan Wagenmakers
    - name: Hang Wu
  coverage:
    - outlet: Science News
      url: https://www.sciencenews.org/blog/science-public/replication-crisis-psychology-science-studies-statistics?tgt=nr
    - outlet: Vox
      url: https://www.vox.com/science-and-health/2018/8/27/17761466/psychology-replication-crisis-nature-social-science
    - outlet: The Atlantic
      url: https://www.theatlantic.com/science/archive/2018/08/scientists-can-collectively-sense-which-psychology-studies-are-weak/568630/
    - outlet: NPR
      url: https://www.npr.org/sections/health-shots/2018/08/27/642218377/in-psychology-and-other-social-sciences-many-studies-fail-the-reproducibility-te?t=1535386468547
    - outlet: Wired
      url: https://www.wired.com/story/social-science-reproducibility/
    - outlet: The Washington Post
      url: https://www.washingtonpost.com/news/speaking-of-science/wp/2018/08/27/researchers-replicate-just-13-of-21-social-science-experiments-published-in-top-journals/?noredirect=on&utm_term=.82fb4a32e969
    - outlet: Nature
      url: https://www.nature.com/articles/d41586-018-06075-z
    - outlet: Science Mag
      url: http://www.sciencemag.org/news/2018/08/generous-approach-replication-confirms-many-high-profile-social-science-findings
    - outlet: The Guardian
      url: https://www.theguardian.com/science/2018/aug/27/attempt-to-replicate-major-social-scientific-findings-of-past-decade-fails
    - outlet: Buzzfeed News
      url: https://www.buzzfeednews.com/article/stephaniemlee/psychology-replication-crisis-studies
    - outlet: Marginal Revolution
      url: https://marginalrevolution.com/marginalrevolution/2018/08/gambling-can-save-science-2.html
  abstract: "Being able to replicate scientific findings is crucial for scientific progress. We replicate 21 systematically selected experimental studies in the social sciences published in Nature and Science between 2010 and 2015. The replications follow analysis plans reviewed by the original authors and pre-registered prior to the replications. The replications are high powered, with sample sizes on average about five times higher than in the original studies. We find a significant effect in the same direction as the original study for 13 (62%) studies, and the effect size of the replications is on average about 50% of the original effect size. Replicability varies between 12 (57%) and 14 (67%) studies for complementary replicability indicators. Consistent with these results, the estimated true-positive rate is 67% in a Bayesian analysis. The relative effect size of true positives is estimated to be 71%, suggesting that both false positives and inflated effect sizes of true positives contribute to imperfect reproducibility. Furthermore, we find that peer beliefs of replicability are strongly related to replicability, suggesting that the research community could predict which results would replicate and that failures to replicate were not the result of chance alone."

- id: munafo2015
  type: Publications
  project: Reproducibility
  title: Using Prediction Markets to Forecast Research Evaluations
  journal: Royal Society Open Science
  year: 2015
  doi: 10.1098/rsos.150287
  coauthors:
    - name: Marcus Munafo
      site: http://www.bristol.ac.uk/expsych/people/marcus-r-munafo/index.html
    - name: Thomas Pfeiffer
    - name: Adam Altmejd
    - name: Emma Heikensten
    - name: Johan Almenberg
    - name: Alexander Bird
    - name: Yiling Chen
    - name: Brad Wilson
    - name: Magnus Johannesson
    - name: Anna Dreber
  abstract: "The 2014 Research Excellence Framework (REF2014) was conducted to assess the quality of research carried out at higher education institutions in the UK over a 6 year period. However, the process was criticized for being expensive and bureaucratic, and it was argued that similar information could be obtained more simply from various existing metrics. We were interested in whether a prediction market on the outcome of REF2014 for 33 chemistry departments in the UK would provide information similar to that obtained during the REF2014 process. Prediction markets have become increasingly popular as a means of capturing what is colloquially known as the ‘wisdom of crowds’, and enable individuals to trade ‘bets’ on whether a specific outcome will occur or not. These have been shown to be successful at predicting various outcomes in a number of domains (e.g. sport, entertainment and politics), but have rarely been tested against outcomes based on expert judgements such as those that formed the basis of REF2014."

- id: altmejd2020siblings
  type: Work in Progress
  project: College Choice
  title: "O Brother, Where Start Thou? Sibling Spillovers on College and Major Choice in Four Countries"
  year: 2020
  wp: https://www.edworkingpapers.com/sites/default/files/ai20-230.pdf
  coauthors:
    - name: Andres Barrios-Fernandez
      site: https://andresbarriosf.github.io
    - name: Marin Drlje
      site:
    - name: Joshua Goodman
      site: https://www.joshua-goodman.com
    - name: Michael Hurwitz
      site:
    - name: Dejan Kovac
      site: https://sites.google.com/site/dejankovacecon/
    - name: Christine Mulhern
      site: https://www.cmulhern.com
    - name: Christopher Neilson
      site: https://christopherneilson.github.io
    - name: Jonathan Smith
      site: https://sites.google.com/site/jonathansmithphd/
  abstract: "Family and social networks are widely believed to influence important life decisions but identifying their causal effects is notoriously difficult. Using admissions thresholds that directly affect older but not younger siblings’ college options, we present evidence from the United States, Chile, Sweden and Croatia that older siblings’ college and major choices can significantly influence their younger siblings’ college and major choices. On the extensive margin, an older sibling’s enrollment in a better college increases a younger sibling’s probability of enrolling in college at all, especially for families with low predicted probabilities of enrollment. On the intensive margin, an older sibling’s choice of college or major increases the probability that a younger sibling applies to and enrolls in that same college or major. Spillovers in major choice are stronger when older siblings enroll and succeed in more selective and higher-earning majors. The observed spillovers are not well-explained by price, income, proximity or legacy effects, but are most consistent with older siblings transmitting otherwise unavailable information about the college experience and its potential returns. The importance of such personally salient information may partly explain persistent differences in college-going rates by geography, income, and other determinants of social networks."

- id: altmejd2018returns
  type: Work in Progress
  project: College Choice
  title: Relative Returns to Swedish College Fields
  year: 2018
  abstract: "This paper studies the economic returns to fields of college education, using a unique Swedish data set of applicant preferences and university admissions. Field payoffs eight years after initial application vary substantially by field, but not nearly as much as in Norway (Kirkebøen et. al., 2016). Medicine, engineering, and business degree holders enjoy positive returns of around $10 000 per year, when compared to most other fields. Humanities, which is the worst paying field, incurs losses of almost as much. These results are causally identified using a regression discontinuity approach and confirmed by admission lotteries. In contrast to many other countries, the findings indicate that Swedish students do not necessarily prefer fields where their potential earnings indicate that they would have a comparative advantage."

- id: altmejd2019predicting
  type: Publications
  project: Reproducibility
  title: Predicting the replicability of social science lab experiments
  year: 2019
  doi: 10.1371/journal.pone.0225826
  journal: PLOS One
  coauthors:
    - name: Anna Dreber
      site: https://sites.google.com/site/annadreber/
    - name: Eskil Forsell
    - name: Gideon Nave
      site: https://marketing.wharton.upenn.edu/profile/gnave/
    - name: Juergen Huber
      site: https://www.uibk.ac.at/ibf/mitarbeiter/huberj.html
    - name: Magnus Johannesson
      site: https://www.hhs.se/en/person/?personid=1981033
    - name: Michael Kirchler
      site: https://www.uibk.ac.at/ibf/team/kirchler.html.en
    - name: Taisuke Imai
      site: https://www.taisukeimai.com
    - name: Teck Ho
      site: http://www.teckho.com/
    - name: Colin Camerer
      site: http://www.its.caltech.edu/~camerer/camerer.html
  abstract: "We measure how accurately replication of experimental results can be predicted by black-box statistical models. With data from four large-scale replication projects in experimental psychology and economics, and techniques from machine learning, we train predictive models and study which variables drive predictable replication. The models predicts binary replication with a cross-validated accuracy rate of 70% (AUC of 0.77) and estimates of relative effect sizes with a Spearman ρ of 0.38. The accuracy level is similar to market-aggregated beliefs of peer scientists (Camerer et al., 2016; Dreber et al., 2015). The predictive power is validated in a pre-registered out of sample test of the outcome of Camerer et al. (2018), where 71% (AUC of 0.73) of replications are predicted correctly and effect size correlations amount to ρ = 0.25. Basic features such as the sample and effect sizes in original papers, and whether reported effects are single-variable main effects or two-variable interactions, are predictive of successful replication. The models presented in this paper are simple tools to produce cheap, prognostic replicability metrics. These models could be useful in institutionalizing the process of evaluation of new findings and guiding resources to those direct replications that are likely to be most informative."
